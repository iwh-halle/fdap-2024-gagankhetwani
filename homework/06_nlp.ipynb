{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![head.png](https://github.com/iwh-halle/FinancialDataAnalytics/blob/master/figures/head.jpg?raw=1)\n",
    "\n",
    "# Financial Data Analytics in Python\n",
    "\n",
    "**Prof. Dr. Fabian Woebbeking**</br>\n",
    "Assistant Professor of Financial Economics\n",
    "\n",
    "IWH - Leibniz Institute for Economic Research</br>\n",
    "MLU - Martin Luther University Halle-Wittenberg\n",
    "\n",
    "fabian.woebbeking@iwh-halle.de"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework: Natural Language Processing (NLP)\n",
    "\n",
    "You will need a Git/GitHub repository to submit your course deliverables. Consult [**slides.ipynb**](https://github.com/iwh-halle/FinancialDataAnalytics) for help with the tasks below! If you need further assistance, do not hesitate to open a Q&A at https://github.com/iwh-halle/FinancialDataAnalytics/discussions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Sourcing\n",
    "\n",
    "The first stage involves sourcing and reading the textual content of scientific papers. You find an example pdf file in ``../lit/nonanswers.pdf``. Please [download](https://scholar.google.de/) and analyze at least one additional paper of your choice (make sure to commit the paper to your repository).\n",
    "\n",
    "Use an appropriate PDF reading library or tool to programmatically extract the text. You can find an example below, however, you are free to use any Python library you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Let me get back to you” –\n",
      "A machine learning approach to measuring\n",
      "non-answers\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install pdfminer.six if you haven't already\n",
    "# You can install it using conda or pip, see \n",
    "  # https://anaconda.org/conda-forge/pdfminer.six\n",
    "  # https://pypi.org/project/pdfminer.six/\n",
    "\n",
    "# Step 2: Import the required module\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "# Step 3: Extract text from PDF file\n",
    "extracted_text = extract_text('../lit/nonanswers.pdf')\n",
    "print(extracted_text[0:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "International Journal on Recent and Innovation Trends in Computing and Communication         \n",
      "           ISSN: 2321-8169 \n",
      "Volume: 2 Issue: 1                                                                                                                                                                                           96 – 100 \n",
      "_______________________________________________________________________________________ \n",
      "\n",
      "Research Paper on Basic of Artificial Neural Network \n",
      "\n",
      "Ms. Sonali. B. Maind \n",
      "Department of Information Technology \n",
      "Datta Meghe Institute of Engineering, Technology & Research, Sawangi (M), Wardha  \n",
      "sonali.maind@gmail.com \n",
      "\n",
      "Ms. Priyanka Wankar \n",
      "Department of Computer Science and Engineering \n",
      "Datta Meghe Institute of Engineering, Technology & Research, Sawangi (M), Wardha \n"
     ]
    }
   ],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "extracted_text = extract_text('../Research_Paper_on_Basic_of_Artificial_Ne.pdf')\n",
    "print(extracted_text[0:800])  # Print the first 80 characters to check the extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Pre-processing\n",
    "\n",
    "Pre-processing is a critical step aimed at cleaning and preparing the text data for analysis. Steps that you should consider:\n",
    "\n",
    "* Removing punctuation, numbers and special characters using regular expressions.\n",
    "* Converting all the text to a uniform case (usually lower case) to ensure that the analysis is not case-sensitive.\n",
    "* Stop word removal, i.e. eliminating commonly used words (e.g., 'and', 'the', 'is') that do not contribute significantly to the overall meaning and can skew the analysis.\n",
    "* Other potential pre-processing steps might include stemming and lemmatization, depending on the specific requirements and goals of the analysis. (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "International Journal on Recent and Innovation Trends in Computing and Communication         \n",
      "           ISSN: 2321-8169 \n",
      "Volume: 2 Issue: 1                                                                                                                                                                                           96 – 100 \n",
      "_______________________________________________________________________________________ \n",
      "\n",
      "Research Paper on Basic of Artificial Neural Network \n",
      "\n",
      "Ms. Sonali. B. Mai\n",
      "\n",
      "Preprocessed Text:\n",
      "international journal recent innovation trend computing communication issn volume issue research paper basic artificial neural network m sonali b maind department information technology datta meghe institute engineering technology research sawangi wardha sonalimaindgmailcom m priyanka wankar department computer science engineering datta meghe institute engineering technology research sawangi wardha priyankawankargmailcom abstractan artificial neural network ann information processing paradigm in\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize stop words and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def remove_punctuation_numbers(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = remove_punctuation_numbers(text)\n",
    "    text = to_lowercase(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = lemmatize_text(text)\n",
    "    return text\n",
    "\n",
    "# Step 1: Specify the path to your PDF file\n",
    "pdf_file_path = '../Research_Paper_on_Basic_of_Artificial_Ne.pdf'\n",
    "\n",
    "# Step 2: Extract text from the PDF file\n",
    "extracted_text = extract_text(\"../Research_Paper_on_Basic_of_Artificial_Ne.pdf\")\n",
    "\n",
    "# Step 3: Preprocess the extracted text\n",
    "preprocessed_text = preprocess_text(extracted_text)\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(extracted_text[0:500])  # Print the first 500 characters of the original text\n",
    "print(\"\\nPreprocessed Text:\")\n",
    "print(preprocessed_text[0:500])  # Print the first 500 characters of the preprocessed text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Analysis\n",
    "\n",
    "The final stage is the analysis of the pre-processed text, in order to extract meaningful context. This may involve:\n",
    "\n",
    "* Frequency Analysis: Determining the most commonly occurring words or phrases, which can provide initial insights into the primary focus areas of the papers. Consider, e.g. a word cloud as a visualization.\n",
    "* Contextual Analysis: Using more advanced NLP techniques such as Word Embedding or Topic Modeling to understand the context of the papers.\n",
    "* Sentiment analysis: We would expect that scientific papers are written in a neutral tone, can you confirm this?\n",
    "* Summarization: Employing algorithms to generate concise summaries of the papers, capturing the key points and findings.\n",
    "\n",
    "Pick any method that you like (you are allowed to use ChatGPT's API as well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessed_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Generate word cloud for the preprocessed text\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m plot_wordcloud(\u001b[43mpreprocessed_text\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocessed_text' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "def plot_wordcloud(text):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Generate word cloud for the preprocessed text\n",
    "plot_wordcloud(preprocessed_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
